---
title: "Learning CHARME models with DNN"
author: by José G. Gómez-García, Jalal Fadili and Christophe Chesneau
output:
  html_notebook: default
  pdf_document: default
  html_document:
    df_print: paged
---

-----------------------------------------------------------------------------------------------------------------------------

Note: this is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 
\
Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

-----------------------------------------------------------------------------------------------------------------------------

In this R Notebook we present the codes in order to reproduce the experiments of the article: [Learning CHARME models with (deep)
neural networks](https://arxiv.org/). 

-----------------------------------------------------------------------------------------------------------------------------

In order to successfully run the experiments below, we need to include all the functions in Requeriments.Rmd and the following packages and libraries
```{r}
library(sigmoid)
install.packages("fBasics")
library(fBasics)
install.packages("devtools")
devtools::install_github("rstudio/keras")
library(keras)
install_keras()
```

-----------------------------------------------------------------------------------------------------------------------------

## Numerical Experiment 1: Training NNs with SGD using a NN-based CHARME data. 
1.1. Generation of the data
\
1.1.1. Generation of the true parameter $\theta^{0}$ of the model:
```{r}
red1 <- sample.model.architecture(c(30,50,60,40,20,1), 0.1) 
red2 <- sample.model.architecture(c(30,20,5,1), 0.09) 
red3 <- sample.model.architecture(c(30,25,30,1), 0.05) 
```
Now, we change the last biases in order to obtain abrupt changes
```{r}
red1[[1]][[length(red1[[1]])]][1,21] <- 1
red2[[1]][[length(red2[[1]])]][1,6] <- 0
red3[[1]][[length(red3[[1]])]][1,31] <- -1
```
We set the probabilities $(\pi_1, \pi_2, \pi_3)=(0.1, 0.4, 0.5)$. Notice that $c=\sum_{k=1}^{3}\pi_k A_k$ must be less than or equal to 1 in order to obtain the stationarity of the model. In fact, 
```{r}
prob <- c(0.1, 0.4, 0.5)
c <- prob%*%c(red1[[2]], red2[[2]], red3[[2]])
c
```
Once we have verified that $c <1$, we forget the $A_k=redk[[2]]$ and we rename the parameters as follows:
```{r}
red1 <- red1[[1]] #theta_1^0
red2 <- red2[[1]] #theta_2^0
red3 <- red3[[1]] #theta_3^0
```
1.1.2. Generation of the time series
```{r}
n <- 100000
X <- c(NA)
p <- ncol(red1[[1]])-1
e <- rnorm(n+100)
X[1:p] <- e[1:p]
S <- Switching(n+100,prob)
for (t in (p+1):(n+100)){
  X[t] <- (rep(S[t],length(prob)) == 1:length(prob)) %*% c(model_function(red1, relu, X[(t-1):(t-p)]),
                                                           model_function(red2, relu, X[(t-1):(t-p)]), 
                                                           model_function(red3, relu, X[(t-1):(t-p)])) + e[t]
}
St <- S[100:length(S)]
Xt <- X[100:length(S)]
plot(Xt[1:2000], col=St, pch='*', ylab = 'X_t', xlab = 't' )
lines(Xt[1:2000],lty=3)
```

-----------------------------------------------------------------------------------------------------------------------------

1.2. Estimation or learning
\
\
We build a matrix $D=[R_t, X_t, (X_{t-1, \ldots, X_{t-p}})] \in \mathbb{M}_{n\times p+2}(\mathbb{R})$ that we then separate according to the states of $R_t$:
```{r}
D <- matrix(NA, ncol = p+2, nrow = n-p)
for (t in 1:(n-p)){
  D[t,] <- c(St[t+p], Xt[(t+p):t])
}
D1 <- D[Index_Switching(D[,1],1),]
D2 <- D[Index_Switching(D[,1],2),]
D3 <- D[Index_Switching(D[,1],3),]
```
\
For each case, we set the NN model and then estimate the weights and biases via the SGD algorithm.
\
\
For the regime or state 1:
```{r}
model1 <- keras_model_sequential()

model1 %>% 
  layer_dense(units = 50, input_shape = 30) %>%
  #layer_dropout(rate=0.25)%>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 60) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 40) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 20) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 1)

model1 %>% compile(
  loss = 'mean_squared_error',
  optimizer = 'sgd', lr = 0.001, decay = 0.5,
  metrics = 'mae'
)

X.train1 <- D1[, 3:ncol(D1)]
Y.train1 <- D1[,2]

model1 %>% fit(X.train1, Y.train1, epochs = 1000)
```
\
For the regime or state 2:
```{r}
model2 <- keras_model_sequential()

model2 %>% 
  layer_dense(units = 20, input_shape = 30) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 5) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 1)

model2 %>% compile(
  loss = 'mean_squared_error',
  optimizer = 'sgd', lr = 0.001, decay = 0.5, 
  metrics = 'mae'
)

X.train2 <- D2[, 3:ncol(D1)]
Y.train2 <- D2[,2]

model2 %>% fit(X.train2, Y.train2, epochs = 1000)

```
\
For the regime or state 3:
```{r}
model3 <- keras_model_sequential()

model3 %>% 
  layer_dense(units = 25, input_shape = 30) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 30) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 1)

model3 %>% compile(
  loss = 'mean_squared_error',
  optimizer = 'sgd', lr = 0.001, decay = 0.5, 
  metrics = 'mae'
)

X.train3 <- D3[, 3:ncol(D1)]
Y.train3 <- D3[,2]

model3 %>% fit(X.train3, Y.train3, epochs = 1000)
```

------------------------------------------------------------------------------------------------------------------------------

1.3. Study of the result of the estimation/learning. 
\

To do this, we first transform the list of weights and biases calculated in Keras format to our format in order to give the estimated autogression functions
```{r}
theta1_t <- T_KG(get_weights(model1))
theta2_t <- T_KG(get_weights(model2))
theta3_t <- T_KG(get_weights(model3))
```
and calculate the errors $\hat{\epsilon}_t=X_t^{(k)}-f_k(X_{t-1}, \ldots, X_{t-p}; \hat{\theta_k})$ according to the case. 
```{r}
hat.e1 <- Y.train1 - apply(X.train1, 1, function(x){model_function(theta1_t, relu, x)})
hat.e2 <- Y.train2 - apply(X.train2, 1, function(x){model_function(theta2_t, relu, x)})
hat.e3 <- Y.train3 - apply(X.train3, 1, function(x){model_function(theta3_t, relu, x)})
hat.e <- c(hat.e1, hat.e2, hat.e3)
```

\
The following figure shows the Gaussian behavior of the errors obtained
```{r}
hist(hat.e, prob=TRUE, breaks=30, ylim=c(0, 0.52), main = '', xlab = 'hat.e_t')
curve(dnorm(x, mean(hat.e), sd(hat.e)), add=TRUE, col="darkblue", lwd=2)
lines(density(hat.e), col='red', lwd=2)
legend(1, 0.5, legend=c("Gaussian density", "Empirical density"), col=c("darkblue", "red"), lty=1, lwd=2)
```

-----------------------------------------------------------------------------------------------------------------------------

## Numerical Experiment 2: Training NNs with SGD using a CHARME data. 
2.1. Generation of the data
\
\
We consider first the following functions:
```{r}
f_ar <- function(ar_parameters, input){
  return(ar_parameters[-1]%*%input + ar_parameters[1])
}
f_arch <- function(arch_parameters, input){
  return(sqrt(arch_parameters[-1]%*%input^2 +  arch_parameters[1]))
}
```
\

So, we generate the data:
```{r}
n <- 100000
X2 <- c(NA)
p <- 5
X2[1:p] <- rnorm(5) 
e2 <- rnorm(n+100)
prob2 <- c(0.15, 0.35, 0.5)
S2 <- Switching(n+100,prob2)
parameter1 <- c(0.1, 0.05, 0.2, 0.15, 0.03, 0.01)
parameter2 <- c(-3, 0.2, 0.1, 0.25, 0.2, 0.05)
c_e2 <- prob2%*%c(1, sum(parameter1[-1]), sum(parameter2[-1]))
c_e2
for (t in (p+1):(n+100)){
  X2[t] <- (rep(S2[t],length(prob2)) == 1:length(prob2)) %*% c(3 + X2[t-1], 
                                                               f_arch(parameter1, X2[(t-1):(t-p)]), 
                                                               f_ar(parameter2, X2[(t-1):(t-p)])) + e2[t]
}
St2 <- S2[100:length(S2)]
Xt2 <- X2[100:length(S2)]
plot(Xt2[1:2000], col=St2, pch='*', ylab = 'X_t', xlab = 't' )
lines(Xt2[1:2000],lty=3)
```

-----------------------------------------------------------------------------------------------------------------------------

2.2. Estimation or learning
\
\
Similarly to 1.2, we build a matrix $E=[R_t, X_t, (X_{t-1, \ldots, X_{t-p}})] \in \mathbb{M}_{n\times (p+2)}(\mathbb{R})$ that we then separate according to the states of $R_t$:
```{r}
E <- matrix(NA, ncol = p+2, nrow = n-p)
for (t in 1:(n-p)){
  E[t,] <- c(St2[t+p], Xt2[(t+p):t])
}
E1 <- E[Index_Switching(E[,1],1),]
E2 <- E[Index_Switching(E[,1],2),]
E3 <- E[Index_Switching(E[,1],3),]
```
\

For each case, we set the NN model and then estimate the weights and biases via the SGD algorithm.
\
\
For the regime or state 1:
```{r}
model1_e2 <- keras_model_sequential()

model1_e2 %>% 
  layer_dense(units = 300, input_shape = 5) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 400) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 200) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 1)

model1_e2 %>% compile(
  loss = 'mean_squared_error',
  optimizer = 'sgd',# lr = 0.00001, # decay = 0.5,
  metrics = 'mae'
)

X.train1_e2 <- E1[, 3:ncol(E1)]
Y.train1_e2 <- E1[,2]

model1_e2 %>% fit(X.train1_e2, Y.train1_e2, epochs = 2000)
```
\

For the regime or state 2:
```{r}
model2_e2 <- keras_model_sequential()

model2_e2 %>% 
  layer_dense(units = 500, input_shape = 5) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 600) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 400) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 1)

model2_e2 %>% compile(
  loss = 'mean_squared_error',
  optimizer = 'sgd', #lr = 0.001, decay = 0.5, 
  metrics = 'mae'
)

X.train2_e2 <- E2[, 3:ncol(E2)]
Y.train2_e2 <- E2[,2]

model2_e2 %>% fit(X.train2_e2, Y.train2_e2, epochs = 2000)
```
\

For the regime or state 3:
```{r}
model3_e2 <- keras_model_sequential()

model3_e2 %>% 
  layer_dense(units = 300, input_shape = 5) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 400) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 200) %>%
  layer_activation(activation = 'relu') %>%
  layer_dense(units = 1)

model3_e2 %>% compile(
  loss = 'mean_squared_error',
  optimizer = 'sgd',# lr = 0.001, decay = 0.5,
  metrics = 'mae'
)

X.train3_e2 <- E3[, 3:ncol(E3)]
Y.train3_e2 <- E3[,2]

model3_e2 %>% fit(X.train3_e2, Y.train3_e2, epochs = 2000)
```

-----------------------------------------------------------------------------------------------------------------------------

2.3. Study of the result of the estimation/learning. 
\

To do this, we first transform the list of weights and biases calculated in Keras format to our format in order to give the estimated autogression functions
```{r}
theta1_e2_t <- T_KG(get_weights(model1_e2))
theta2_e2_t <- T_KG(get_weights(model2_e2))
theta3_e2_t <- T_KG(get_weights(model3_e2))
```
\
and calculate the errors $\hat{\epsilon}_t=X_t^{(k)}-f_k(X_{t-1}, \ldots, X_{t-p}; \hat{\theta_k})$ according to the case 
```{r}
hat.e1.2 <- Y.train1_e2 - apply(X.train1_e2, 1, function(x){model_function(theta1_e2_t, relu, x)})
hat.e2.2 <- Y.train2_e2 - apply(X.train2_e2, 1, function(x){model_function(theta2_e2_t, relu, x)})
hat.e3.2 <- Y.train3_e2 - apply(X.train3_e2, 1, function(x){model_function(theta3_e2_t, relu, x)})
hat.e.2 <- c(hat.e1.2, hat.e2.2, hat.e3.2)
```

\
The following figure shows the Gaussian behavior of the errors obtained
```{r}
hist(hat.e.2, prob=TRUE, breaks=40, ylim = c(0, 1.6), main = '', xlab = 'hat.e_t')
curve(dnorm(x, mean(hat.e.2), sd(hat.e.2)), add=TRUE, col="darkblue", lwd=2)
lines(density(hat.e.2), col='red', lwd=2)
legend(0.5, 1.5, legend=c("Gaussian density", "Empirical density"), col=c("darkblue", "red"), lty=1, lwd=2)
```

-----------------------------------------------------------------------------------------------------------------------------

## Numerical Experiment 3: Asymptotic normality of trained NNs.
3.1. Generation of the data
\
3.1.1. Generation of the true parameter $\theta^{0}$ of the model:
```{r}
red1 <- sample.model.architecture(c(16,32,64,32,1), 0.08) 
red2 <- sample.model.architecture(c(16,64,32,1), 0.05)
red3 <- sample.model.architecture(c(16,32,64, 1), 0.07)
```

\
Now, we change the last biases in order to obtain abrupt changes
```{r}
red1[[1]][[length(red1[[1]])]][1,21] <- 1
red2[[1]][[length(red2[[1]])]][1,6] <- 0
red3[[1]][[length(red3[[1]])]][1,31] <- -1
```

\
We set the probabilities $(\pi_1, \pi_2, \pi_3)=(0.4, 0.5, 0.1)$. Notice that $c=\sum_{k=1}^{3}\pi_k A_k$ must be less than or equal to 1 in order to obtain the stationarity of the model. In fact, 
```{r}
prob <- c(0.4, 0.5, 0.1)
c <- prob%*%c(red1[[2]], red2[[2]], red3[[2]])
c
```

\
Once we have verified that $c <1$, we forget the $A_k=redk[[2]]$ and we rename the parameters as follows:
```{r}
red1 <- red1[[1]]
red2 <- red2[[1]]
red3 <- red3[[1]]

red1_0 <- unlist(red1)
red2_0 <- unlist(red2)
red3_0 <- unlist(red3)
n.parameters <- length(c(red1_0, red2_0, red3_0))
```
Here, n.parameters is a vector that contains all the true parameters $\theta^0$. 

-----------------------------------------------------------------------------------------------------------------------------

3.2. Monte Carlo simulation 
\
Now, we calculate $N=$n.samplings times $\hat{\theta}_n$, with $n=20000$, the estimator of the true parameter $\theta^0$. The $N$ estimated parameters are arranged in a matrix "theta.centered", where the number of columns is the size of the parameters space and the number of rows is the number of samples of Monte Carlo.  
```{r}
n.samplings <- 125
n <- 20000
n.iterations.bp <- 2000 #This is the number of iteration of the SGD algorithm.
p <- ncol(red1[[1]])-1
theta.centered <- matrix(NA, ncol = n.parameters, nrow = n.samplings)
for (i in 1:n.samplings){
  print(paste("Iteration = ", i))
  X <- c(NA)
  e <- rnorm(n+100)
  X[1:p] <- e[1:p]
  S <- Switching(n+100,prob)
  for (t in (p+1):(n+100)){
    X[t] <- (rep(S[t],length(prob)) == 1:length(prob))%*%c(model_function(red1, sigmoid, X[(t-1):(t-p)]),
                                                             model_function(red2, sigmoid, X[(t-1):(t-p)]), 
                                                             model_function(red3, sigmoid, X[(t-1):(t-p)]))+e[t]
  }
  St <- S[101:length(S)]
  Xt <- X[101:length(S)]
  # Estimation
  D <- matrix(NA, ncol = p+2, nrow = n-p)
  for (t in 1:(n-p)){
    D[t,] <- c(St[t+p], Xt[(t+p):t])
  }
  #---
  D1 <- D[Index_Switching(D[,1],1),]
  
  model1 <- keras_model_sequential()
  
  model1 %>% 
    layer_dense(units = 32, input_shape = 16) %>%
    layer_activation(activation = 'sigmoid') %>%
    layer_dense(units = 64) %>%
    layer_activation(activation = 'sigmoid') %>%
    layer_dense(units = 32) %>%
    layer_activation(activation = 'sigmoid') %>%
    layer_dense(units = 1)
  
  model1 %>% compile(
    loss = 'mean_squared_error',
    optimizer = 'sgd', #lr = 0.001, decay = 0.5,
    #metrics = 'mae'
  )
  
  X.train1 <- D1[, 3:ncol(D1)]
  Y.train1 <- D1[,2]
  
  model1 %>% fit(X.train1, Y.train1, epochs = 3*n.iterations.bp/2)
  
  theta1.centered <- (unlist(T_KG(get_weights(model1))) - red1_0)*sqrt(n-p)
  #-----------------------------------
  D2 <- D[Index_Switching(D[,1],2),]
  
  model2 <- keras_model_sequential()
  
  model2 %>% 
    layer_dense(units = 64, input_shape = 16) %>%
    layer_activation(activation = 'sigmoid') %>%
    layer_dense(units = 32) %>%
    layer_activation(activation = 'sigmoid') %>%
    layer_dense(units = 1)
  
  model2 %>% compile(
    loss = 'mean_squared_error',
    optimizer = 'sgd', #lr = 0.001, decay = 0.5,
    #metrics = 'mae'
  )
  
  X.train2 <- D2[, 3:ncol(D2)]
  Y.train2 <- D2[,2]
  
  model2 %>% fit(X.train2, Y.train2, epochs = n.iterations.bp)
  
  theta2.centered <- (unlist(T_KG(get_weights(model2))) - red2_0)*sqrt(n-p)
  #--------------------------------
  D3 <- D[Index_Switching(D[,1],3),]
  
  model3 <- keras_model_sequential()
  
  model3 %>% 
    layer_dense(units = 32, input_shape = 16) %>%
    layer_activation(activation = 'sigmoid') %>%
    layer_dense(units = 64) %>%
    layer_activation(activation = 'sigmoid') %>%
    layer_dense(units = 1)
  
  model3 %>% compile(
    loss = 'mean_squared_error',
    optimizer = 'sgd', #lr = 0.001, decay = 0.5,
    #metrics = 'mae'
  )
  
  X.train3 <- D3[, 3:ncol(D3)]
  Y.train3 <- D3[,2]
  
  model3 %>% fit(X.train3, Y.train3, epochs = n.iterations.bp)
  
  theta3.centered <- (unlist(T_KG(get_weights(model3))) - red3_0)*sqrt(n-p)
  
  #-----------------------------------------------------------------------------------
  theta.centered[i, ] <- c(theta1.centered, theta2.centered, theta3.centered)
}
```

---------------------------------------------------------------------------------------------------------------------------------

3.3. Multivariate Test Normality
\
Finally we apply several tests in order to study the normality of the estimator. 
\
\
3.3.1. The boxplots of a subset of $100$ parameters.
```{r}
set <- sample(1:ncol(theta.centered), 100)
boxplot(theta.centered[, set], boxwex=0.5, xlab='100 arbitrarily chosen parameters')
abline(h=0,lty=9, col='red')
```

\
3.3.2. The test of Mardia, Royston and Henze-Zirkler for a subset of $15$ parameters. 
```{r}
set2 <- sample(1:ncol(theta.centered), 15)
```

\
Mardia's Test:
```{r}
result1 <- mardiaTest(theta.centered[, set2],qqplot=FALSE)
result1
```

\
Royston's Test:
```{r}
result2 <- roystonTest(theta.centered[, set2], qqplot = FALSE )
result2
```

\
Henze-Zirkler's Test:
```{r}
result3 <- hzTest(theta.centered[, set2], qqplot= TRUE)
result3
```